{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36487546",
   "metadata": {},
   "source": [
    "# Logit lens visualization for GPT-2-XL\n",
    "\n",
    "First load the model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67160ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, baukit\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "MODEL_NAME = \"gpt2-xl\"  # gpt2-xl or EleutherAI/gpt-j-6B\n",
    "model, tok = (\n",
    "    AutoModelForCausalLM.from_pretrained(MODEL_NAME, low_cpu_mem_usage=False).to(\"cuda\"),\n",
    "    AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1251ea",
   "metadata": {},
   "source": [
    "We're not training the model, so set the requires_grad=False flav on all the model parameters, so that pytorch autograd isn't automatically invoked every time we use the parameters.  (The alternative is to `torch.set_grad_enabled(False)` to turn off autograd globally.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c82626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import baukit\n",
    "baukit.set_requires_grad(False, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6681f96",
   "metadata": {},
   "source": [
    "Here is an example of the API for the model.  It takes as input `{'input_ids': tensor, 'attention_mask': tensor}`, and it returns a dictionary where the main output is `full_out['logits']`, and the main next prediction token is given by the very last logit.\n",
    "\n",
    "If we want to do greedy generation, we can take the next predicted token, append it to the input ids, and iterate.  That is what this function does.  (There are much more tricky ways to generate beyond this that lead to more natural output text; this is just a demonstration of the simplest approach.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440d3266",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, tok, prefix, n=10):\n",
    "    inp = {k: torch.tensor(v)[None].cuda() for k, v in tok(prefix).items()}\n",
    "    initial_length = len(inp['input_ids'][0])\n",
    "    pkv = None\n",
    "    for _ in range(n):\n",
    "        full_out = model(**inp)\n",
    "        out = full_out['logits']\n",
    "        pred = out[0, -1].argmax()\n",
    "        inp['input_ids'] = torch.cat((inp['input_ids'], torch.tensor([pred])[None].cuda()), dim=1)\n",
    "        inp['attention_mask'] = torch.cat((inp['attention_mask'], torch.ones(1, 1).cuda()), dim=1)\n",
    "    return tok.decode(inp['input_ids'][0, initial_length:])\n",
    "generate(model, tok, 'In his NBA career, KC Jones played', n=100)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca9dbf4",
   "metadata": {},
   "source": [
    "The layer ouput activations that we're interested in tracing are called `transformer.h.0`, `transformer.h.1` etc.\n",
    "\n",
    "This function demonstrates how to use `baukit.TraceDict` to gather those activations while we are running `model` in inference.  Note that `tr[layername].output` has the exact output of the layer, which is a tuple of tensors.  The first tensor in this tuple happens to be the main hidden-state tensor that we are looking for.\n",
    "\n",
    "We stack all the `tr[layername].output[0]` tensors into one big tensor, 48 long, for all layers' hidden states gathered in one place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2989295d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hidden_states(model, tok, prefix):\n",
    "    import re\n",
    "    from baukit import TraceDict\n",
    "    inp = {k: torch.tensor(v)[None].cuda() for k, v in tok(prefix).items()}\n",
    "    layer_names = [n for n, _ in model.named_modules()\n",
    "                   if re.match(r'^transformer.h.\\d+$', n)]\n",
    "    with TraceDict(model, layer_names) as tr:\n",
    "        logits = model(**inp)['logits']\n",
    "    return torch.stack([tr[layername].output[0] for layername in layer_names])\n",
    "\n",
    "prompt = 'Hello, my name is also'\n",
    "hs = get_hidden_states(model, tok, prompt)\n",
    "hs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c4aa29",
   "metadata": {},
   "source": [
    "Here is the basic logit lens visualization.  Comments inline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10aff42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_logit_lens(model, tok, prefix, topk=5, color=None, hs=None):\n",
    "    from baukit import show\n",
    "\n",
    "    # You can pass in a function to compute the hidden states, or just the tensor of hidden states.\n",
    "    if hs is None:\n",
    "        hs = get_hidden_states\n",
    "    if callable(hs):\n",
    "        hs = hs(model, tok, prefix)\n",
    "\n",
    "    # The full decoder head normalizes hidden state and applies softmax at the end.\n",
    "    decoder = torch.nn.Sequential(model.transformer.ln_f, model.lm_head, torch.nn.Softmax(dim=-1))\n",
    "    probs = decoder(hs) # Apply the decoder head to every hidden state\n",
    "    favorite_probs, favorite_tokens = probs.topk(k=topk, dim=-1)\n",
    "    # Let's also plot hidden state magnitudes\n",
    "    magnitudes = hs.norm(dim=-1)\n",
    "    # For some reason the 0th token always has huge magnitudes, so normalize based on subsequent token max.\n",
    "    magnitudes = magnitudes / magnitudes[:,:,1:].max()\n",
    "    \n",
    "    # All the input tokens.\n",
    "    prompt_tokens = [tok.decode(t) for t in tok.encode(prefix)]\n",
    "\n",
    "    # Foreground color shows token probability, and background color shows hs magnitude\n",
    "    if color is None:\n",
    "        color = [0, 0, 255]\n",
    "    def color_fn(m, p):\n",
    "        a = [int(255 * (1-m) + c * m) for c in color]\n",
    "        b = [int(255 * (1-p) + 0 * p)] * 3\n",
    "        return show.style(background=f'rgb({a[0]}, {a[1]}, {a[2]})',\n",
    "                          color=f'rgb({b[0]}, {b[1]}, {b[2]})' )\n",
    "\n",
    "    # In the hover popup, show topk probabilities beyond the 0th.\n",
    "    def hover(tok, prob, toks, m):\n",
    "        lines = [f'mag: {m:.2f}']\n",
    "        for p, t in zip(prob, toks):\n",
    "            lines.append(f'{tok.decode(t)}: prob {p:.2f}')\n",
    "        return show.attr(title='\\n'.join(lines))\n",
    "    \n",
    "    # Construct the HTML output using show.\n",
    "    header_line = [ # header line\n",
    "             [[show.style(fontWeight='bold'), 'Layer']] +\n",
    "             [\n",
    "                 [show.style(background='yellow'), show.attr(title=f'Token {i}'), t]\n",
    "                 for i, t in enumerate(prompt_tokens)\n",
    "             ]\n",
    "         ]\n",
    "    layer_logits = [\n",
    "             # first column\n",
    "             [[show.style(fontWeight='bold'), layer]] +\n",
    "             [\n",
    "                 # subsequent columns\n",
    "                 [color_fn(m, p[0]), hover(tok, p, t, m), show.style(overflowX='hide'), tok.decode(t[0])]\n",
    "                 for m, p, t in zip(wordmags, wordprobs, words)\n",
    "             ]\n",
    "        for layer, wordmags, wordprobs, words in\n",
    "                zip(range(len(magnitudes)), magnitudes[:, 0], favorite_probs[:, 0], favorite_tokens[:,0])]\n",
    "    \n",
    "    # If you want to get the html without showing it, use show.html(...)\n",
    "    show(header_line + layer_logits + header_line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae7d715",
   "metadata": {},
   "source": [
    "An example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bbda64",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_logit_lens(model, tok, '. The Space Needle is located in')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2929eec6",
   "metadata": {},
   "source": [
    "Now instead of directly decoding each hidden state, let's decocde the deltas between layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9249e5a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_hidden_state_deltas(model, tok, prefix):\n",
    "    import re\n",
    "    from baukit import TraceDict\n",
    "    inp = {k: torch.tensor(v)[None].cuda() for k, v in tok(prefix).items()}\n",
    "    layer_names = [n for n, _ in model.named_modules()\n",
    "                   if re.match(r'^transformer.h.\\d+$', n)]\n",
    "    with TraceDict(model, ['transformer.drop'] + layer_names) as tr:\n",
    "        logits = model(**inp)['logits']\n",
    "    first_h = tr['transformer.drop'].output[None]\n",
    "    other_h = torch.stack([tr[ln].output[0] for ln in layer_names])\n",
    "    all_h = torch.cat([first_h, other_h])\n",
    "    delta_h = all_h[1:] - all_h[:-1]\n",
    "    return delta_h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bc15af",
   "metadata": {},
   "source": [
    "Test the delta function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590a795b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hs = get_hidden_state_deltas(model, tok, 'The Space Needle is located in')\n",
    "hs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94250d95",
   "metadata": {},
   "source": [
    "Now plot it.\n",
    "\n",
    "Notice!  \"Seattle\" shows up at layer 29 at token 4!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e932ef16",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_logit_lens(model, tok, '. The Space Needle is located in', hs=get_hidden_state_deltas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e227367",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(model.forward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
